{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyORfZDdpxm5EmGbMdTGVYl9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rahulrama6705/perplexity/blob/main/perplexity.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LAB 2 -Write a Python Program to compute BLEU and perplexity scores for n-gram and RNN Language models."
      ],
      "metadata": {
        "id": "jKVw40Aq8lsb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk torch torchtext\n",
        "\n",
        "import nltk\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "from nltk.util import ngrams\n",
        "from collections import Counter\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "nltk.download('punkt')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ehxQ6VdE7wQ4",
        "outputId": "a7acc676-ac54-4b6a-aa22-222545293a96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
            "Collecting torchtext\n",
            "  Downloading torchtext-0.18.0-cp312-cp312-manylinux1_x86_64.whl.metadata (7.9 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2025.11.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from torchtext) (2.32.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchtext) (2.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->torchtext) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->torchtext) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->torchtext) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->torchtext) (2025.11.12)\n",
            "Downloading torchtext-0.18.0-cp312-cp312-manylinux1_x86_64.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m35.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torchtext\n",
            "Successfully installed torchtext-0.18.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Small sample corpus\n",
        "corpus = [\n",
        "    \"the cat sat on the mat\",\n",
        "    \"the dog sat on the mat\",\n",
        "    \"the cat lay on the rug\",\n",
        "    \"the dog lay on the rug\"\n",
        "]\n",
        "\n",
        "tokenized_corpus = [nltk.word_tokenize(sent) for sent in corpus]\n",
        "tokenized_corpus\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QdPWHOYa77dO",
        "outputId": "40f25099-f45f-419c-cf8f-41f73e1a2f08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['the', 'cat', 'sat', 'on', 'the', 'mat'],\n",
              " ['the', 'dog', 'sat', 'on', 'the', 'mat'],\n",
              " ['the', 'cat', 'lay', 'on', 'the', 'rug'],\n",
              " ['the', 'dog', 'lay', 'on', 'the', 'rug']]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def build_ngram_model(tokenized_texts, n=2):\n",
        "    model = Counter()\n",
        "    total = 0\n",
        "    for sent in tokenized_texts:\n",
        "        for gram in ngrams(sent, n):\n",
        "            model[gram] += 1\n",
        "            total += 1\n",
        "    return model, total\n",
        "\n",
        "bigram_model, bigram_total = build_ngram_model(tokenized_corpus, n=2)\n",
        "trigram_model, trigram_total = build_ngram_model(tokenized_corpus, n=3)\n"
      ],
      "metadata": {
        "id": "gngj9YZI8Df4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ngram_perplexity(test_sentence, model, total_count, n=2):\n",
        "    tokens = nltk.word_tokenize(test_sentence)\n",
        "    grams = list(ngrams(tokens, n))\n",
        "    log_prob_sum = 0\n",
        "    vocab = len(model)\n",
        "\n",
        "    for gram in grams:\n",
        "        count = model.get(gram, 0)\n",
        "        prob = (count + 1) / (total_count + vocab)   # Add-1 smoothing\n",
        "        log_prob_sum += math.log(prob)\n",
        "\n",
        "    perplexity = math.exp(-log_prob_sum / len(grams))\n",
        "    return perplexity\n",
        "\n",
        "test_sentence = \"the cat lay on the mat\"\n",
        "print(\"Bigram PP:\", ngram_perplexity(test_sentence, bigram_model, bigram_total, n=2))\n",
        "print(\"Trigram PP:\", ngram_perplexity(test_sentence, trigram_model, trigram_total, n=3))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eq16v5XR8Esh",
        "outputId": "89ad26e2-8d2f-4a80-d3de-0a50ec73b988"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bigram PP: 10.117866411063428\n",
            "Trigram PP: 11.430952132988166\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reference = [nltk.word_tokenize(\"the cat sat on the mat\")]\n",
        "candidate = nltk.word_tokenize(\"the cat lay on the mat\")\n",
        "\n",
        "smooth = SmoothingFunction().method1\n",
        "bleu_score = sentence_bleu(reference, candidate, smoothing_function=smooth)\n",
        "\n",
        "print(\"BLEU Score:\", bleu_score)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GMU619Sr8IEM",
        "outputId": "60103e62-d9c7-4b7f-cf3e-12a6378c3262"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BLEU Score: 0.25406637407730737\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class RNNLanguageModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim=32, hidden_dim=64):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.rnn = nn.RNN(embed_dim, hidden_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embeds = self.embed(x)\n",
        "        out, _ = self.rnn(embeds)\n",
        "        logits = self.fc(out)\n",
        "        return logits\n"
      ],
      "metadata": {
        "id": "SFeH8VH78KRb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build vocab\n",
        "all_tokens = [word for sent in tokenized_corpus for word in sent]\n",
        "vocab = list(set(all_tokens))\n",
        "word2idx = {w:i for i,w in enumerate(vocab)}\n",
        "idx2word = {i:w for w,i in word2idx.items()}\n",
        "\n",
        "def encode(sentence):\n",
        "    return torch.tensor([word2idx[w] for w in nltk.word_tokenize(sentence)])\n",
        "\n",
        "# Training data: next-word prediction\n",
        "inputs, targets = [], []\n",
        "\n",
        "for sent in tokenized_corpus:\n",
        "    encoded = encode(\" \".join(sent))\n",
        "    inputs.append(encoded[:-1])\n",
        "    targets.append(encoded[1:])\n",
        "\n",
        "inputs = nn.utils.rnn.pad_sequence(inputs, batch_first=True)\n",
        "targets = nn.utils.rnn.pad_sequence(targets, batch_first=True)\n"
      ],
      "metadata": {
        "id": "KHHL-l4g8Ll6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = RNNLanguageModel(len(vocab))\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "epochs = 50\n",
        "for epoch in range(epochs):\n",
        "    optimizer.zero_grad()\n",
        "    logits = model(inputs)\n",
        "    loss = criterion(logits.view(-1, len(vocab)), targets.view(-1))\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "print(\"Training Complete — Final Loss:\", loss.item())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7jSooMzA8OX7",
        "outputId": "bdaf05d6-db21-43d6-e8b5-b0b7869b195c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Complete — Final Loss: 0.27784213423728943\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def rnn_perplexity(model, sentence):\n",
        "    encoded = encode(sentence)\n",
        "    input_seq = encoded[:-1].unsqueeze(0)\n",
        "    target_seq = encoded[1:].unsqueeze(0)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits = model(input_seq)\n",
        "        loss = criterion(logits.view(-1, len(vocab)), target_seq.view(-1))\n",
        "\n",
        "    return math.exp(loss.item())\n",
        "\n",
        "print(\"RNN Perplexity:\", rnn_perplexity(model, test_sentence))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GSEd5dfk8RZK",
        "outputId": "d5d1b104-06c9-4e00-fb59-2d827f1b5139"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RNN Perplexity: 5.968755708905813\n"
          ]
        }
      ]
    }
  ]
}